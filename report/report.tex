\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}

\renewcommand{\thesection}{\arabic{section}}

\begin{document}

\title{Project AI \\ Auto-Encoding Variational Bayes}
\author{	
	Joost van Amersfoort \\ 10021248  
	\and
	Otto Fabius \\ 5619858
	}
\maketitle

\begin{abstract}
In this paper, we use the recently developed unsupervised learning method of Auto-Encoding Variational Bayes (Kingma and Welling, 2014) on various datasets. For each dataset, we learn the parameters for both an encoder and a decoder, implemented as two-layer neural networks, allowing for encoding data of the same type and generating new datapoints from (calculated or sampled) hidden representations. We apply the learned auto-encoders for compression, classification, (.......). Several results from Kingma and Welling (2014) were reproduced, as well as ........
\end{abstract}

\section{AEVB}

\subsection*{Problem class}

The problem class associated with AEVB is that where data $\mathbf{X}$ is assumed to be produced by some underlying process involving (continuous) hidden variables $\mathbf{Z}$. We can express the posterior probability over $\mathbf{X}$ by reformulating Bayes' Rule:

\begin{align}
P(X) = \frac{P(X|Z)P(Z)}{P(Z|X)}
\end{align}

Where $P(X|Z)$ and $P(Z|X)$ are parameterized by $\theta$.
\\

Knowing the probability distributions $P(X|Z)$ and $P(Z|X)$, i.e. learning $\theta$, can sometimes provide information on some natural process (if one is interested in the values of the parameters, for example). But knowing the distribution $P(Z|X)$ also allows us to infer $\mathbf{Z}$ from $\mathbf{X}$ to reduce the dimensionality of (encode) our data and find structure in the data. Furthermore, introducing a prior $P(Z)$ allows for computing the marginal probability $P(X)$ to generate data for applications such as image inpainting, denoising and super-resolution.
We would like to learn $\theta$ by gradient ascent/descent by differentiating some objective w.r.t. $\theta$. This can be done by choosing the marginal likelihood $P(X) = \int \frac{P(Z)}{P(X|Z)}dZ$ as objective to maximize, or by using the EM algorithm on the posterior density $ P(Z|X) = \frac{P(X|Z)P(Z)}{P(X)}$. However, calculating either  the marginal likelihood or the posterior density becomes intractable as the likelihood function $P(X|Z)$	 becomes more complex, which severely limits these methods. (possible e.g. variational linear regression, Bishop p. 486)





%In the class of problems we are interested in, the following conditions apply:
%\begin{itemize}
%\item The posterior distribution $P(Z|X)$ is intractable. 
%\item the probability density functions of the prior $P(Z)$ and the likelihood $P(X|Z)$, respectively, are differentiable w.r.t. $\mathbf{Z}$ and $\theta$. 
%\end{itemize}

\subsection{Sampling to obtain gradients}

Introducing $q(Z|X)$, parameterized by $\phi$, as an approximation of the posterior $P(Z|X)$, Kingma and Welling (2014) derive an expression for the Monte Carlo estimate of the lower bound $\mathcal{L}$ of the marginal likelihood $P(X)$:

\begin{align}
\mathcal{L}(\theta ,\phi ,  \mathbf{x^{(i)}}) \backsimeq \frac{1}{L} \sum_{l=1}^{L} \text{log } p_{\theta} (x^{(i)}|z^{(l)})+ \text{log }p(z^{(l)}	)-\text{log }q_{\phi}(z^{(l)}|x^{(i)})
\end{align}

Here, $z$ is sampled $l$ times from $q_{\phi}(z|x)$ for each datapoint $x^{(i)}$. Because $z$ therefore depends on parameters $\theta$, differentiating $\mathcal{L}$ w.r.t. $\phi$ will lead to a gradient which is influenced by the current parameters $\phi$. 
As a solution to this problem, Kingma and Welling (2014) reparameterize the samples of $z$ from $q(z|x)$ as
\begin{align}
z = g_\phi(\mathbf{\epsilon},\mathbf{x}) \text{  with  } \mathbf{\epsilon} \sim p(\mathbf{\epsilon}) 
\end{align} 

Now, once we have a sample $\epsilon$, the MC-estimate of $\mathcal{L}$ (equation (1) ) does not depend on a \textit{sampled} $z$, but on a  $z$ which is \textit{calculated deterministically}, independent of $\phi$. Thus, $\mathcal(L)$ can now be differentiated w.r.t all the parameters $\theta$ and $\phi$, enabling parameter optimization by means of stochastic gradient ascent.

\subsection{Modelling the conditional distributions}

For our experiments, we approximate $P(X|Z)$ and $q(X|Z)$ as neural networks with a single hidden layer, similar to Kingma and Welling (2014). These neural networks can be trained well with backpropagation of error derivatives. They are also powerful, in the sense that they are universal approximators, i.e. they can approximate any function of their input. 

IETS OVER BINARY/CONTINUOUS OUTPUT \\
IETS MEER OVER NEURAL NETWORKS OID? IIG REFS...

This way, once we have trained the two networks, we have an \textit{encoder} that maps $\mathbf{x}$ to its hidden variables $\mathbf{z}$, and a (generative) \textit{decoder} that maps any combination of hidden variables to a (new) datapoint $\mathbf{x}$. This has a strong similarity to auto-encoders (refs??). 

The encoder allows us to compress and/or organize data. Inpainting, denoising and super-resolution (up to the resolution on which is trained) can be realized by generating the hidden representation from the datapoint with the encoder, and creating the desired output with the decoder.

\section{Experiments}

In this section, we outline our experiments and present our results. We will present and discuss our results immediately after the description of each experiment so it is clear to the reader which results and conclusion belong to which experiment.

\subsection{Reproducing results}

Firstly, we performed some experiments to reproduce the results of (ref...). In their paper, an Auto-Encoder was trained on both data consisting of distinct classes (MNIST handwritten digits) and continuous data (Frey Faces).

\subsubsection{MNIST}

\subsubsection{Frey Faces}

\subsection{Classification}

In the remainder of the experiments, we will focus on one useful application of AEVB classification. In particular, we will examine the performance of logistic regression on various representations of the same dataset. For logistic regression, we can use a trained Auto-Encoder to extract features from the data by calculating the output of the hidden layer of the Auto-Encoder. This way, we hop to improve the results of classification (supervised learning) with AEVB (unsupervised learning).

\subsubsection{Classifying MNIST on features}

compare baseline log regression and on RBM, AE, Double AE

\subsubsection{Classifying Chinese characters}

log regression op pixels, single AE features, double AE features (?)

\subsubsection{Classification on small datasets}

In practice it is hard to obtain large annotated datasets for supervised learning. Therefore, we also tested the effectiveness of features calculated from a trained Auto-Encoder for improving the results of logistic regression on small subsets of MNIST (and chinese characters?). 

MNIST - resultaten al op docs

\section*{Conclusion}



\section*{References}


\end{document}
