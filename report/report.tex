\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}

\renewcommand{\thesection}{\arabic{section}}

\begin{document}

\title{Project AI \\ Auto-Encoding Variational Bayes}
\author{	
	Joost van Amersfoort \\ 10021248  
	\and
	Otto Fabius \\ 5619858
	}
\maketitle

\section*{Introduction}
background literature \\ 


\section*{AEVB}

\subsection*{Problem class}

The problem class associated with AEVB is that where data $\mathbf{X}$ is assumed to be produced by some underlying process involving (continuous) hidden variables $\mathbf{Z}$. We can express the posterior probability over $\mathbf{X}$ by reformulating Bayes' Rule:

\begin{align*}
P(X) = \frac{P(X|Z)P(Z)}{P(Z|X)}
\end{align*}

Where $P(X|Z)$ and $P(Z|X)$ are parameterized by $\theta$.
\\

Knowing the probability distributions $P(X|Z)$ and $P(Z|X)$, i.e. learning $\theta$, can sometimes provide information on some natural process (if one is interested in the values of the parameters, for example). But knowing the distribution $P(Z|X)$ also allows us to infer $\mathbf{Z}$ from $\mathbf{X}$ to reduce the dimensionality of (encode) our data and find structure in the data. Furthermore, introducing a prior $P(Z)$ allows for computing the marginal probability $P(X)$ to generate data for applications such as image inpainting, denoising and super-resolution.
We would like to learn $\theta$ by gradient ascent/descent by differentiating some objective w.r.t. $\theta$. This can be done by choosing the marginal likelihood $P(X) = \int \frac{P(Z)}{P(X|Z)}dZ$ as objective to maximize, or by using the EM algorithm on the posterior density $ P(Z|X) = \frac{P(X|Z)P(Z)}{P(X)}$. However, calculating either  the marginal likelihood or the posterior density becomes intractable as the likelihood function $P(X|Z)$	 becomes more complex, which severely limits these methods.






In the class of problems we are interested in, the following conditions apply:
\begin{itemize}
\item The posterior distribution $P(Z|X)$ is intractable. 
\item the probability density functions of the prior $P(Z)$ and the likelihood $P(X|Z)$, respectively, are differentiable w.r.t. $\mathbf{Z}$ and $\theta$. 
\end{itemize}

\subsection*{Sampling to obtain gradients}



\subsection*{Modelling the conditional distributions}

\section*{Experiments}

Describe algorithms,datasets, param choices, and motivate.

\section*{Results}

Shiny pictures with captions

\section*{Discussion}

what worked well/what didn't? Why? Possible improvements/further directions for research?

\section*{Conclusion}



\section*{References}


\end{document}
