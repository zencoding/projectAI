\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}

\renewcommand{\thesection}{\arabic{section}}

\begin{document}

\title{Project AI \\ Auto-Encoding Stochastic Gradient Variational Bayes}
\author{	
	Joost van Amersfoort \\ 10021248  
	\and
	Otto Fabius \\ 5619858
	}
\maketitle

\section{Introduction}

In Machine Learning, it is often useful to model data as being generated from some underlying process, or hidden variables, according to some parametrized model. A powerful way to determine these parameters and latent variables (commonly grouped together as "unobserved variables") is to perform Bayesian Inference in order to obtain the MAP estimate of the parameters and full posterior of the latent variables. The integral required in regular EM and mean-field approaches is, however, intractable, and is in practice approximated by a family of techniques known as Variational Bayes. These techniques have various drawbacks such as limited scalability or limited performance. \\
Recently, Kingma and Welling \cite{kingma2013auto} submitted a paper to ICLR 2014, where they present a new Variational Bayesian method, of which experimental results show it to be effective and scalable to large datasets. In this report, we describe the novelty that is introduced and investigate several applications. Specifically, we try to replicate the results of Kingma and Welling by implementing the auto-encoder that they describe in the paper. After this replication step, that confirms our implementation works as it should, we test if using the auto-encoder to transform our data improves classification. Furthermore, we test the hypothesis that performing classification on the features is robust to overfitting. 

\section{Stochastic Gradient Variational Bayes}

\subsection{Problem class}

The problem class associated with SGVB is that where data $\mathbf{X}$ is assumed to be produced by some underlying process involving (continuous) hidden variables $\mathbf{Z}$. We can express the marginal probability over $\mathbf{X}$ by reformulating Bayes' Rule:

\begin{align}
P(X) = \frac{P(X|Z)P(Z)}{P(Z|X)}
\end{align}

Where $P(X|Z)$ and $P(Z|X)$ are parameterized by $\theta$.
\\

Knowing the probability distributions $P(X|Z)$ and $P(Z|X)$, i.e. learning $\theta$, can sometimes provide information on some natural process (if one is interested in the values of the parameters, for example). But knowing the distribution $P(Z|X)$ also allows us to infer $\mathbf{Z}$ from $\mathbf{X}$ to reduce the dimensionality of (encode) our data and find structure in the data. Furthermore, introducing a prior $P(Z)$ allows for computing the marginal probability $P(X)$ to generate data for applications such as image inpainting, denoising and super-resolution.
We would like to learn $\theta$ by gradient ascent/descent by differentiating some objective w.r.t. $\theta$. This can be done by choosing the marginal likelihood $P(X) = \int_z P(Z) P(X|Z) dZ$ as objective to maximize, or by using the EM algorithm on the posterior density $ P(Z|X) = \frac{P(X|Z)P(Z)}{P(X)}$. However, calculating either  the marginal likelihood or the posterior density becomes intractable as the likelihood function $P(X|Z)$ becomes more complex, which severely limits these methods.

\subsection{The variational bound}

In order to be able to find the likelihood function, \cite{kingma2013auto} introduce $q_\phi(z|x)$ which approximates the (intractable) true posterior $p_\theta(z|x)$. The marginal likelihood of each datapoint i then becomes:

\begin{align}
	\log p_\theta(x^{(i)}) = D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}^{(i)}) || p_\theta(\mathbf{z}|\mathbf{x}^{(i)})) + \mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{x}^{(i)})
\end{align} 

Where $\mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{x}^{(i)})$ is called the variational lower bound on the marginal likelihood. This can be rewritten as:

\begin{align}
\mathcal{L}(\mathbf{\theta}, \mathbf{\phi}; \mathbf{x}^{(i)}) = D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}^{(i)}) || p_\theta(\mathbf{z}|\mathbf{x}^{(i)})) + \mathcal(E)_{q_\phi(z|x^{(i)})} [\log p_\theta(x^{(i)}|z)]
\end{align}

Now, to be able to optimize the lowerbound, we need to be able to differentiate it with respect to both $\phi$ and $\theta$. However the gradient with respect to $\phi$ is not possible. As a solution to this problem, \cite{kingma2013auto} reparameterizes the samples of $z$ from $q(z|x)$ as
\begin{align}
z = g_\phi(\mathbf{\epsilon},\mathbf{x}) \text{  with  } \mathbf{\epsilon} \sim p(\mathbf{\epsilon}) 
\end{align} 
Where $p(\mathbf{\epsilon}) = N(0,I)$.

%volgende zin snap ik (nog?) niet
%Ik snap dit gedeelte ook niet helemaal, staat in de paper onderaan pagina 3 van versie 7: http://arxiv.org/pdf/1312.6114v7.pdf
%Misschien nog even overleggen
We can now form Monte Carlo estimates of expectations of some function f(z) with respect to $q_\phi(z|x)$. This yields the following expression for the lower bound:

\begin{align}
\mathcal{L}(\theta ,\phi ,  \mathbf{x^{(i)}}) \backsimeq \frac{1}{L} \sum_{l=1}^{L} \text{log } p_{\theta} (x^{(i)}|z^{(l)})+ \text{log }p(z^{(l)}	)-\text{log }q_{\phi}(z^{(l)}|x^{(i)})
\end{align}

\cite{kingma2013auto} derive an analytical expression for KLD, which we also use in our implementation as this reduces noise and computation time. On this lower bound we can perform gradient descent and make our 


\subsection{Modelling the conditional distributions}
In our experiments, $q(Z|X)$ and $P(X|Z)$ are neural networks with a single hidden layer, similar to \cite{kingma2013auto}.  Obtaining the parameters for the Gaussian MLP $q(\mathbf{z}|\mathbf{x})$ from one data point $\mathbf{x}$ is now done through:
\begin{align}
\mathbf{\mu_z} = \mathbf{W}_2^T \cdot f(\mathbf{W}_1^T\mathbf{x}+\mathbf{b_1})+\mathbf{b_2} \\
\log\mathbf{(\sigma_z}^2) = \mathbf{W}_3^T \cdot f(\mathbf{W}_1^T\mathbf{x}+\mathbf{b_1})+\mathbf{b_3}
\end{align} 
 
Where $f$ denotes the activation function of the hidden layer.\\
If the data $\mathbf{X}$ is continuously valued, $P(\mathbf{X}|\mathbf{Z})$ can also be modeled as Gaussian MLP, but for binary valued data, $P(\mathbf{X}|\mathbf{Z})$ is modeled as a Bernoulli MLP. These neural networks are powerful, in the sense that they are universal approximators, i.e. they can approximate any function of their input. They can be efficiently trained with backpropagation of error derivatives, especially if batch optimization is used. This is done by expanding $x$, $\mathbf{\mu_z}$ and $\mathbf{\sigma_z}$ to matrices to acquire gradients of multiple data points at once for one parameter update step.\\
Training these two MLPs in such a fashion effectively creates an (variational) auto-encoder, where a hidden representation, or features, can be computed through the \textit{encoder} $q(\mathbf{z}|\mathbf{x})$ and data can be generated from a hidden representation $\mathbf{z}$ through the \textit{decoder} $p(\mathbf{x}|\mathbf{z})$

\section{Auto-encoders}

Before we dive into our experiments, let us first discuss Auto-Encoders, or autoassociators as \cite{bengio2009learning} calls them, in some more detail. Auto-Encoders are trained to encode the input in some representation so that the input can be reconstructed from that representation. In a simple network with one hidden layer that has a linear activation function and is trained with the mean squared error criterion, the hidden units learn the principal components of the data (PCA). However, if the hidden units have a non-linear activation function, the auto-encoder can learn a more powerful mapping \cite{japkowicz2000nonlinear}. \\
Auto-encoders are frequently used for dimensionality reduction. Also, feature extraction is a useful application. Extracting useful features from data is useful in different areas, one example being classification.

An important application of auto-encoders, as \cite{bengio2009learning} describe, is as a first step in constructing deep multi-layer neural networks. An example of this application is to first train an auto-encoder on unlabeled data, then train another auto-encoder on the output of the hidden units of the first auto-encoder. This step can be repeated as many times as necessary. This yields a deep network with good initialization of the weights, which can be fine-tuned through backpropagation of error derivatives. 

However, for many classes of probabilistic models there is no tractable estimator for the log-likelihood. Therefore, in practice Restricted Boltzmann Machines are used as building blocks and the network is called a Deep Belief Network, as described in detail in \cite{hinton2006reducing}. SGVB auto-encoders could prove to be a viable, theoretically attractive alternative for RBMs, not only in this application, but in general.

\section{Experiments}

In this section, we outline our experiments and present our results. We will present and discuss our results immediately after the description of each experiment so it is clear to the reader which results and conclusion belong to which experiment.

\subsection{Reproducing results}

Firstly, we performed some experiments to reproduce the results of \cite{kingma2013auto}. This way, we can verify the correctness of our implementation. In their paper, an Auto-Encoder was trained on both binary valued data (MNIST handwritten digits) and continuous valued data (Frey Faces \footnote{Available at http://www.cs.nyu.edu/ Ìƒroweis/data.html}).

\subsubsection{MNIST}

We first trained an auto-encoder with 400 hidden units and 20-dimensional latent space with SGVB on MNIST, in order to compare the lower bound of the log likelihood with \cite{kingma2013auto}. Figure 1 shows the lower bound per data point during training for both the training set and the test set. Figure 2 shows a manifold of generated data from an auto-encoder trained two-dimensional latent space and 400 hidden units. To create this visualization it is first necessary to create a 2D grid of vectors of the latent variables, such that each vector represents an equal probability mass from $p(z)$. Each data point was then generated by calculating the mean outputs of the trained auto-encoder on the values of the grid. The position in the total image reflects the coordinates of the hidden space. \\ 

\begin{figure}[htb]
\begin{center}
\includegraphics[height=3.1in,width=4in]{lowerboundAEVBMNIST.png}
\caption{Lower bound of the log likelihood per data point of both the train and test data during training}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[height=4in,width=5in]{manifoldMNIST.png}
\caption{2D Manifold of MNIST}
\end{center}
\end{figure}

\subsubsection{Frey Faces}

Also for the continuous valued Frey Face dataset, we trained an auto-encoder with SGVB to compare the progression of the lower bound to that in \cite{kingma2013auto}. Figure 3 shows the lower bound of the training and test set as training progresses for latent dimensionality of 10, with 200 hidden units. Figure 4 shows a manifold of the Frey Face dataset, created in a similar fashion to what we describe in 4.1.1.

\begin{figure}[htb]
\centering
\begin{minipage}{0.5\textwidth}
\includegraphics[height=3in,width=3.6in]{lowerboundFF.png}
\caption{Lower bound of the log likelihood per data point of the Frey Face dataset during training}
\end{minipage}%
\centering
\begin{minipage}{0.5\textwidth}
\includegraphics[height=3.2in,width=3.2in]{manifoldFF.png}\caption{2D Manifold of Frey Face}
\end{minipage}
\end{figure}

Training an auto-encoder with 200 hidden units and 2 latent dimensions once again enables visualization of generated data along both dimensions, shown in Figure 4.

These results achieved with our implementation of SGVB auto-encoders are very similar to those to those presented in \cite{kingma2013auto}, which shows that our implementation is correct and that these results can be achieved consistently. 

\subsection{Classification}

In the remainder of the experiments, we will focus on one useful application of learned features, namely in classification. Before we use a classification method, we can use a trained Auto-Encoder to extract features from the data by calculating the output of the hidden layer of the Auto-Encoder. This way, we aim to improve the results of classification (supervised learning) with SGDVB (unsupervised learning). In particular, we will examine the performance of logistic regression on various representations of the same dataset. Classifying on features calculated by a one-layer neural network effectively results in a two-layered MLP, of which the first layer is not trained with supervised learning after training it with SGDVB. We compare obtained results to those obtained when using features trained with a Restricted Boltzmann Machine, which is a popular method \cite{bengio2009learning} for such feature extraction and is frequently used for weight initialization in layers of deep belief nets (e.g. \cite{bengio2007greedy} ).

\subsubsection{Classifying MNIST on features}

In this subsection, we detail how well logistic regression performed on MNIST when using raw pixel data as input, features learned with an SGVB Auto-Encoder as input, and with features learned with an RBM as input. \\ For this, we used the same Auto-Encoder of which the lower bound is shown in Figure 1. Thus, it has 400 hidden units and latent dimensionality of 20, was trained on approximately $5.0\cdot 10^7$ training examples, and reached a lower bound of $-101.53$ on the log likelihood per data point. The trained RBM also contained 400 hidden units to ensure the same representational power, and was trained on $1.5\cdot 10^7$ training examples from the MNIST training set. This yielded the same results as $0.5\cdot 10^7$ training examples and thus was enough iterations for convergence. This auto-encoder is used in several experiments later. \\
Logistic regression was run until results on the validation set declined or stabilized. The results are shown in Table 1.

\begin{table}
\begin{tabular}{|l|c|c|r|}
\hline
& Raw Data & SGVB AE Features & RBM Features \\ \hline
No. of training examples until convergence ($\cdot 10^6$) & 1.5 & 4.0 & 3.0 \\ \hline 
Test set accuracy & 93.36 & 97.54 & 96.29 \\ \hline
\end{tabular}
\end{table}

As expected, using features calculated with the trained RBM improved the test set accuracy. However, the results using the trained auto-encoder were even better, leading to an error rate of only 2.46\%! This is an important finding, as auto-encoding SGVB proves to be a at least viable alternative to using an RBM for unsupervised learning. Besides outperforming RBMs (at least for this task), AE SGVB is theoretically more attractive than RBMs as it has a clearer objective that is maximized, and allows for calculating the lower bound, allowing for (efficient) monitoring during optimization and comparison  of lower bound across various parameter settings.\\
As the features were extracted from a one-layer neural network, this result was effectively achieved with a two-layer neural network, and it compares well to the 'classic' results achieved with two-layer NNs by \cite{lecun1998gradient}, who was only able to further improve results by deskewing. In this experiment, the first layer was not trained beyond unsupervised learning. Training both layers of the neural network during supervised learning could therefore further improve results.

\subsubsection{Classifying Chinese characters}

To investigate whether the boost in classification accuracy generalizes (at least in part) to other data, we also performed classification on the handwritten Chinese character dataset \cite{liu2011casia}. \\
This dataset consists of 1,121,749 annotated data points in 3,750 different classes, with each class containing approximately the same number of characters. Each data point contains grey values of an image of a handwritten Chinese character and a code for its corresponding class. For preprocessing, we binarized the images and scaled them to 40x40. This is still larger than MNIST, which has dimensions 28 x 28 but was deemed necessary to keep the more fine grained structure of the characters intact. As images had varying aspect ratios, we added white padding whenever necessary, in order to have constant dimensionality of the data. Due to resource and time limitations, we only used 50 classes, resulting in 11,578 data points. The resulting dataset is similar to MNIST in size and type, but considerably more complex in that the images are larger, there are 5 times as many classes, and the structure in the images is more complex.\\
Once again, we trained an auto-encoder on the data, and used logistic regression to classify the data. For the auto-encoder, we used 500 hidden units and hidden dimensionality of 200. The auto-encoder is trained on the 10,000 images and the lower bound on the marginal log likelihood during training is shown in Figure 5. Due to the amount of classes the test set performance is significantly lower, at around $10^7$ datapoints the performance on the test set no longer increases.


\begin{figure}[htb]
\begin{center}
\includegraphics[height=3.1in,width=4in]{lowerboundchinese.png}
\caption{Lower bound of the log likelihood per data point during training.}
\end{center}
\end{figure}

Once again, we trained a classifier with logistic regression on both the preprocessed pixel values as well as the features extracted from the trained encoder. \\
Logistic regression on the pixel values yielded a test set accuracy of 66.29\%, compared to 69.17\% when classifying on the features. Percentage-wise, this is a similar increase in accuracy as achieved on MNIST. However, as logistic regression on MNIST already scores above 90\%, the improvement there is more impressive. \\
It is not obvious whether this indicates whether the learned features are less effective for this dataset than for MNIST. If so, one possible explanation is that the lower bound had not converged as much as the lower bound for MNIST (see Figure 1). Additional experiments, such as training the auto-encoder longer, training an RBM to extract features to classify on, and training on a larger part of the Chinese character dataset could provide more insight in the effectiveness on SGVB auto-encoders for use in classification in general and for the Chinese character dataset in particular.


\subsubsection{Classification on Small Datasets}

In practice it is hard to obtain large annotated datasets for supervised learning. Therefore, we also tested the effectiveness of learned features for improving the classification results for small subsets of MNIST. Once again, logistic regression is our classification method of choice. In addition to features calculated with the auto-encoder trained earlier (see Figure 1), a two-layer auto-encoder was trained with SGVB. This was done by stacking another auto-encoder on top of the first auto-encoder. This was done simply by using the features, computed by the first auto-encoder, as input for SGD to train a new auto-encoder as before. Since the values of these computed features are continuous, as opposed to the pixel data, the second auto-encoder was trained to approximate Gaussian distribution instead of a Bernoulli distribution, as discussed earlier. For the second auto-encoder, once again 400 hidden units and latent dimensionality of 20 were chosen to be able to compare the results. The resulting features for the two-layer auto-encoder are then computed through the following formula. Note that the output of the first hidden layer is normalized between 0 and 1.

\begin{align}
    f(\textbf{X}) = \log(1 + \exp[-(\tanh(\textbf{W}^T*\textbf{X} + \textbf{b}) + 1)/2])
\end{align}

In our experiment, we selected the first $x$ training examples from the training set of MNIST as our training set, where $x$ started at 4 and was increased each time by multiplying it with 2, up to a size of 2048. To evaluate the results for each dataset size, we used the first 1000 data points of the MNIST test set and validation set, respectively.\\
Figure 6 shows the accuracy on the test set after training logistic regression on the first $x$ data points of MNIST until accuracy on the validation set converged. The results show that training on the features computed with the one-layer auto-encoder yield better results across the whole range of used dataset sizes, in addition to using the full dataset (see 3.2.1). The two-layer auto-encoder shows an additional boost in performance for dataset sizes of up to 64 total data points, but is less effective for larger datasets. \\

\begin{figure}[htb]
\begin{center}
\includegraphics[height=4in,width=5in]{logreg_smallsets_report.png}
\caption{Lower bound of the log likelihood per datapoint of the train set during training.}
\end{center}
\end{figure}

These results show that also for small datasets, unsupervised learning with SGVB auto-encoders can increase the effectiveness of classification algorithms. As in logistic regression on the full dataset, the resulting two- and three-layer MLPs were not trained as a whole on supervised data. Also, the layers of the two-layer auto-encoder were trained independently, while jointly training all the parameters could fine tune the parameters to obtain better features. These possibilities can be explored further in future experiments. Also, training and effectively fine-tuning multilayer auto-encoders with SGVB seems very promising from these somewhat preliminary results. 

\subsection{Generalizability of Features}
%Added for Durk - check
One of the advantages of extracting a set of relevant features from data, in general, is that performing classification on these features is more robust to overfitting than performing classification on the original data. This robustness comes from the fact that the unsupervised learning has extracted meaningful regularity in the raw data, effectively ensuring the classification algorithm does not fit to noise. Figure 7 shows some of the regularity discovered in the data by visualizing the weights of the decoder.

\begin{figure}[htb]
\label{weights}
\centering
\begin{minipage}{0.36\textwidth}
\includegraphics[height=2in,width=2.5in]{w1.png}
\end{minipage}%
\centering
\begin{minipage}{0.36\textwidth}
\includegraphics[height=2in,width=2.5in]{w2.png}
\end{minipage}\\
\centering
\begin{minipage}{0.36\textwidth}
\includegraphics[height=2in,width=2.5in]{w3.png}
\end{minipage}%
\centering
\begin{minipage}{0.36\textwidth}
\includegraphics[height=2in,width=2.5in]{w4.png}
\end{minipage}
\caption{Visualization of four dimension of the weights of the decoder of MNIST}
\end{figure}


In this subsection, we detail an experiment designed to test the extent to whether this general property of features is applicable to the feature representation generated with the trained auto-encoder described earlier.\\
We chose a Support Vector Machine with a Radial Basis Function kernel as our classification method, since the susceptibility of this method to overfitting is easily manipulable by setting the precision of the RBF kernel. Too high values for this hyperparameter will lead to overfitting, while too low values will prevent overfitting, but at the cost of reducing the discriminative power (and thus, accuracy). As the robustness to overfitting is particularly desirable when training on a small dataset, we trained a Support Vector Machine on small subsets of the training set of MNIST, as well as on the features extracted from the same data using the trained auto-encoder. Once again, we used the first 1000 examples from the MNIST test set to calculate the test accuracy.\\
The achieved train and test set accuracy across different dataset sizes are shown for different settings of the precision of the RBF kernel in Figure 8. It is clearly visible that for larger precision, classifying on pixel values is heavily outperformed by using the learned features. Using the features yields acceptable results even with $\gamma = 0.3$, where this leads to such severe overfitting on the pixel values that the resulting classifier barely performs better than random on the test set.\\
These results clearly demonstrate the robustness to overfitting that comes from learning a feature representation of data. This further improves the utility of SGVB as a method to improve classification algorithms, especially when annotated data is scarce. Presumably, these results are compounded when higher-level features are extracted from the data. Once higher-level features have been extracted effectively with SGVB, this experiment can be repeated to investigate this further.




\begin{figure}[htb]
\centering
\begin{minipage}{0.36\textwidth}
\includegraphics[height=2in,width=2.5in]{svm001.png}
\end{minipage}%
\centering
\begin{minipage}{0.36\textwidth}
\includegraphics[height=2in,width=2.5in]{svm003.png}
\end{minipage}\\
\centering
\begin{minipage}{0.36\textwidth}
\includegraphics[height=2in,width=2.5in]{svm01.png}
\end{minipage}%
\centering
\begin{minipage}{0.36\textwidth}
\includegraphics[height=2in,width=2.5in]{svm03.png}
\end{minipage}
\caption{Results of classification on MNIST with a SVM with RBF kernel. Precision of the RBF kernel ($\gamma$) is set to different values, as denoted above each plot.}
\end{figure}

\subsection{Regularizing effect of the lower bound}
Kingma and Welling \cite{kingma2013auto} claim that the lower bound has a regularizing effect. This regularization leads to weights along some hidden dimensions to become negligibly small (unused). This would be a welcome property as it would become unnecessary to tune the amount of hidden dimensions manually. Furthermore, a practical advantage of this is that by inspecting the L2 norm of the weights during training, it is possible to monitor the relative influence of each dimension. This information can provide speed-ups by pruning the unused dimensions during training.

We performed an experiment in which we trained two auto-encoders with 400 hidden units on MNIST. One of these has an approximately appropriate hidden dimensionality of 20, the other one 400, which is too many for MNIST. A histogram of normalized size of the weights across different hidden dimensions is shown in Figure 9.

\begin{figure}[htb]
    \centering
    \begin{minipage}{0.36\textwidth}
        \includegraphics[height=2in,width=2.5in]{relevant_z_N20.png}
    \end{minipage}
    \centering
    \begin{minipage}{0.36\textwidth}
        \includegraphics[height=2in,width=2.5in]{relevant_z_N400.png}
    \end{minipage}
    \caption{Sum of squares of the dimensions of W4 (weights for decoder)}
\end{figure}

In these two histograms it can clearly be seen that around 20 dimensions have a significant size in both auto-encoders. Apparently, this is sufficient to encode information present in the MNIST data. The additional dimensions do not substantially change the result, as the weights of those dimensions are negligible.

\pagebreak
\section*{Conclusion}

In our experiments, we found that the results as described by \cite{kingma2013auto} are indeed reproducible by implementing the theory as laid out in the paper. We also conclude that performing classification on the features learned by the auto-encoder gives a higher score for MNIST as well as Chinese characters. Notably, the boost in performance is higher than what was achievable with RBMs. Another conclusion we can draw from our results is that training on features of the auto-encoder is more robust to overfitting. 

The results presented are encouraging for further research on using SGVB auto-encoders for classification, but also for other tasks. It would be interesting to find out how the variational auto-encoder performs on more complicated problems and with more complicated classification methods. Also, extracting higher level features from data through constructing deeper and otherwise more complex architectures, and fine-tuning these networks as a whole, is a viable extension to the current work. Given the theoretical advantages of the variational auto-encoder over RBMs and the shown improved results, it is possible that this will perform better than the current state-of-the-art techniques. Moreover, constructed auto-encoders have more applications than classification, such as denoising and superpixeling, which we have not yet investigated. \\ 
Of course, auto-encoders are only one area of application for SGVB, and not only will other known areas of application of Variational Bayes be investigated further, but no doubt new areas of application will arise in the future.

\pagebreak 
\nocite{*}
\bibliographystyle{amsplain}
\bibliography{references.bib}

\end{document}
