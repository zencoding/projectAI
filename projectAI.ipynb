{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Lab 2: Classification\n",
      "\n",
      "* Luka Stout - 10616713\n",
      "* Otto Fabius - 5619858\n",
      "* Joost van Amersfoort - 10021248"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import gzip, cPickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_mnist():\n",
      "\tf = gzip.open('mnist.pkl.gz', 'rb')\n",
      "\tdata = cPickle.load(f)\n",
      "\tf.close()\n",
      "\treturn data\n",
      "\n",
      "(x_train, t_train), (x_valid, t_valid), (x_test, t_test) = load_mnist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def plot_digits(data, numcols, shape=(28,28)):\n",
      "    numdigits = data.shape[0]\n",
      "    numrows = int(numdigits/numcols)\n",
      "    for i in range(numdigits):\n",
      "        plt.subplot(numrows, numcols, i)\n",
      "        plt.axis('off')\n",
      "        plt.imshow(data[i].reshape(shape), interpolation='nearest', cmap='Greys')\n",
      "    plt.show()\n",
      "    \n",
      "plot_digits(x_train[0:12], numcols=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
      "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
      "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
      "$\\newcommand{\\by}{\\mathbf{y}}$\n",
      "$\\newcommand{\\bm}{\\mathbf{m}}$\n",
      "$\\newcommand{\\bb}{\\mathbf{b}}$\n",
      "$\\newcommand{\\bS}{\\mathbf{S}}$\n",
      "$\\newcommand{\\ba}{\\mathbf{a}}$\n",
      "$\\newcommand{\\bz}{\\mathbf{z}}$\n",
      "$\\newcommand{\\bv}{\\mathbf{v}}$\n",
      "$\\newcommand{\\bq}{\\mathbf{q}}$\n",
      "$\\newcommand{\\bp}{\\mathbf{p}}$\n",
      "$\\newcommand{\\bh}{\\mathbf{h}}$\n",
      "$\\newcommand{\\bI}{\\mathbf{I}}$\n",
      "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
      "$\\newcommand{\\bT}{\\mathbf{T}}$\n",
      "$\\newcommand{\\bPhi}{\\mathbf{\\Phi}}$\n",
      "$\\newcommand{\\bW}{\\mathbf{W}}$\n",
      "$\\newcommand{\\bV}{\\mathbf{V}}$\n",
      "##1.1.1\n",
      "\n",
      "####Complete the above derivations for $\\delta^q_j$ by furtherly developing $\\frac{\\partial \\log Z}{\\partial Z}$ and $\\frac{\\partial Z}{\\partial \\log q_j}$.\n",
      "\\begin{align}\n",
      "    \\frac{\\partial \\log Z}{\\partial Z} \\frac{\\partial Z}{\\partial \\log q_j}\n",
      "     = \n",
      "\\end{align}\n",
      "\n",
      "For $j = t^{(i)}$:\n",
      "$\n",
      "\\delta^q_j\n",
      "= 1 - \\frac{e^{\\mathbf{w}_j^Tx+b_j}}{Z}\n",
      "$\n",
      "\n",
      "For $j \\neq t^{(i)}$:\n",
      "$\n",
      "\\delta^q_j\n",
      "= - \\frac{e^{\\mathbf{w}_j^Tx+b_j}}{Z}\n",
      "$\n",
      "\n",
      "####What is $\\frac{\\partial \\log q_j}{\\partial W_{ij}}$? Complete the equation above.\n",
      "$$\n",
      "\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial W_{ij}} =\n",
      "\\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\log q_j}\n",
      "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
      "= \\mathbf{\\delta}_j^q\n",
      "\\frac{\\partial \\log q_j}{\\partial W_{ij}}\n",
      "$$\n",
      "\n",
      "So taking the partial derivative will leave just x:\n",
      "\n",
      "$\n",
      "\\nabla_{\\mathbf{w_j}} \\mathcal{L}^{(i)} = \\mathbf{\\delta}_j^q x \n",
      "$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.1.2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def logreg_gradients(x,t,w,b):\n",
      "    logq = w.T.dot(x) + b\n",
      "        \n",
      "    p = np.exp(logq - np.log(np.sum(np.exp(logq))))\n",
      "    \n",
      "    deltab = -p\n",
      "    deltab[t] = 1-p[t]\n",
      "    \n",
      "    deltaw = np.outer(x,deltab)\n",
      "    \n",
      "    return (deltaw,deltab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.1.3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_iter(x_train, t_train, w, b):\n",
      "    learningrate = 0.0001\n",
      "    all_indices = np.arange(len(x_train),dtype=int)\n",
      "    np.random.shuffle(all_indices)\n",
      "        \n",
      "    for i in all_indices:\n",
      "        deltaw,deltab = logreg_gradients(x_train[i],t_train[i],w,b)\n",
      "        w += learningrate * deltaw\n",
      "        b += learningrate * deltab\n",
      "                \n",
      "    return (w,b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#1.2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Help functions for the exercises\n",
      "\n",
      "def train(iterations):\n",
      "    w = np.zeros([784,10])\n",
      "    b = np.zeros([10])\n",
      "    \n",
      "    listW = []\n",
      "    listB = []\n",
      "    for i in xrange(iterations):\n",
      "        w,b = sgd_iter(x_train,t_train,w,b)\n",
      "        listW.append(w.copy())\n",
      "        listB.append(b.copy())\n",
      "    return listW,listB\n",
      "\n",
      "def check(x,t,w,b):\n",
      "    logq = w.T.dot(x) + b\n",
      "    p = np.exp(logq - np.log(np.sum(np.exp(logq))))\n",
      "    \n",
      "    return (np.argmax(p) == t, np.amax(p))\n",
      "\n",
      "def checkSet(X,T,w,b):\n",
      "    probDict = dict()\n",
      "    cL = []\n",
      "    for i in xrange(len(X)):\n",
      "        correctness, probability = check(X[i],T[i],w,b)\n",
      "        probDict[i] = probability\n",
      "        cL.append(correctness)\n",
      "    return probDict,cL"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.2.1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iterations = 10\n",
      "\n",
      "listW, listB = train(iterations)\n",
      "probTrain = []\n",
      "probValid = []\n",
      "for i in xrange(iterations):\n",
      "    prob, cL = checkSet(x_train,t_train,listW[i],listB[i])\n",
      "    probTrain.append(sum(np.log(prob.values())))\n",
      "    #print \"Train error:\", 1-len([c for c in cL if c])/float(len(cL))\n",
      "    prob,cL = checkSet(x_valid,t_valid,listW[i],listB[i])\n",
      "    probValid.append(sum(np.log(prob.values())))\n",
      "    #print \"Valid error:\", 1-len([c for c in cL if c])/float(len(cL))\n",
      "    \n",
      "plt.plot(probTrain, label=\"Cond. Prob. Train\")\n",
      "plt.plot(probValid, label=\"Cond. Prob. Valid\")\n",
      "plt.legend(loc = \"lower right\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.2.2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listW, listB = train(5)\n",
      "plot_digits(listW[-1].T,5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1.2.3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "listW, listB = train(5)\n",
      "\n",
      "probDict, cL = checkSet(x_valid,t_valid,listW[-1],listB[-1])\n",
      "\n",
      "easyX = sorted(probDict, key=probDict.get, reverse=True)[:12]\n",
      "hardX = sorted(probDict, key=probDict.get, reverse=False)[:12]\n",
      "\n",
      "print \"Easy\"\n",
      "easyL = []\n",
      "for i in easyX:\n",
      "    easyL.append(x_test[i])\n",
      "plot_digits(np.array(easyL), numcols = 4)\n",
      "\n",
      "print \"Hard\"\n",
      "hardL = []\n",
      "for i in hardX:\n",
      "    hardL.append(x_test[i])\n",
      "plot_digits(np.array(hardL), numcols = 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2.1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####State (shortly) why $\\nabla_{\\bb} \\mathcal{L}^{(i)}$ is equal to the earlier (multiclass logistic regression) case, and why $\\nabla_{\\bw_j} \\mathcal{L}^{(i)}$ is almost equal to the earlier case.\n",
      "\n",
      "$\\nabla_{\\bb} \\mathcal{L}^{(i)}$ is the gradient of the bias of the weights. Since these are applied after input has gone through the neural network, it won't change from the earlier case. $\\nabla_{\\bw_j} \\mathcal{L}^{(i)}$ is almost the same, but the weights work on the output of the hidden layer (h) instead of directly on the input x, and the dimension of that output is different. \n",
      "\n",
      "####Give the equations for computing $\\mathbf{\\delta}^h$, and for computing the derivatives of $\\mathcal{L}^{(i)}$ w.r.t. $\\bW$, $\\bb$, $\\bV$ and $\\ba$. \n",
      "\n",
      "\\begin{align}\n",
      "    \\mathbf{\\delta}^h_j = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\mathbf{h}_j} \n",
      "    = \\frac{\\partial\\mathcal{L}}{\\partial\\log \\mathbf{q}} \\frac{\\partial\\log \\mathbf{q}}{\\partial \\mathbf{h}_j}\n",
      "\\end{align}\n",
      "\n",
      "Here we have to make a distinction again for when $j = t$ and when $j \\neq t$, for brevity sake we left out all the steps that can be found in 1.1.1. For $j = t$:\n",
      "\\begin{align}\n",
      "     \\mathbf{\\delta}^h_j = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\mathbf{h}_j} \n",
      "      = (1- \\frac{\\mathbf{w}^T_jh_j+b_j}{\\mathbf{Z}}) \\cdot \\mathbf{w}^T_j\n",
      "\\end{align}\n",
      "For $j \\neq t$:\n",
      "\\begin{align}\n",
      "     \\mathbf{\\delta}^h_j = \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\mathbf{h}_j} \n",
      "      = (- \\frac{\\mathbf{w}^T_jh_j+b_j}{\\mathbf{Z}}) \\cdot \\mathbf{w}^T_j\n",
      "\\end{align}\n",
      "\n",
      "The result for $\\mathbf{W}$ is very similar:\n",
      "\\begin{align}\n",
      "    \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\mathbf{W}_{ij}} \n",
      "    = \\frac{\\partial\\mathcal{L}}{\\partial\\log \\mathbf{q}} \\frac{\\partial\\log \\mathbf{q}}{\\partial \\mathbf{W}_{ij}}\n",
      "\\end{align}\n",
      "For $j = t$:\n",
      "\\begin{align}\n",
      "    \\nabla_{\\bW_j} \\mathcal{L}^{(i)}\n",
      "      = (1- \\frac{\\mathbf{w}^T_jh_j+b_j}{\\mathbf{Z}}) h_j\n",
      "\\end{align}\n",
      "For $j \\neq t$:\n",
      "\\begin{align}\n",
      "     \\nabla_{\\bW_j} \\mathcal{L}^{(i)}\n",
      "      = (- \\frac{\\mathbf{w}^T_jh_j+b_j}{\\mathbf{Z}}) h_j\n",
      "\\end{align}\n",
      "\n",
      "The result of a is again very similar to the result of a:\n",
      "\\begin{align}\n",
      "    \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_j} = \\frac{\\partial\\mathcal{L}}{\\partial\\log \\mathbf{q}} \\frac{\\partial\\log \\mathbf{q}}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}\\frac{\\partial \\mathbf{x}}{\\partial a_j}\n",
      "    = \\mathbf{\\delta}^h_j * \\sigma (x)(1-\\sigma (x))\n",
      "\\end{align}\n",
      "\n",
      "\n",
      "The result of V follows from there:\n",
      "\\begin{align}\n",
      "    \\nabla_{\\mathbf{V}_j} \\mathcal{L}^{(i)} = \\frac{\\partial\\mathcal{L}}{\\partial\\log \\mathbf{q}} \\frac{\\partial\\log \\mathbf{q}}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{v_j}}\n",
      "    = \\mathbf{\\delta}^h_j * \\sigma (x)(1-\\sigma (x)) \\cdot \\mathbf{x}\n",
      "\\end{align}\n",
      "\n",
      "\n",
      "\n",
      "$\\frac{\\partial \\mathcal{L}}{\\partial b_j}$ is of course equal to what we already found in 1.1.1:\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2.2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first step is to apply a Laplace aproximation to initialize the hyperparameter $\\alpha$, and then to determine the parameter vector \\bw by maximizing the log posterior distribution.\n",
      "\n",
      "This is equivalent to minimizing the regularized error function:\n",
      "    $$E(w) = -\\ln p (\\mathcal{D} \\mid w) + \\frac{\\alpha}{2}w^Tw$$\n",
      "    \n",
      "Of which the first part (the log likelihood) is already implemented and the second part can be done with back propagation and for example gradient descent."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2.3.1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mlp_gradients(x,t,W,b,V,a):\n",
      "    h = 1 / (1 + np.exp(-(V.T.dot(x) + a)))\n",
      "    \n",
      "    logq = W.T.dot(h) + b\n",
      "    \n",
      "    p = np.exp(logq - np.log(np.sum(np.exp(logq))))\n",
      "    \n",
      "    ## Create delta b (equal to delta^q and gradient of L w.r.t. b)\n",
      "    deltab = -p\n",
      "    deltab[t] = 1-p[t]\n",
      "    \n",
      "    ## Create delta W\n",
      "    deltaw = np.outer(h,deltab)\n",
      "           \n",
      "    ##Create h, the updated hidden layer\n",
      "    deltah = W.dot(deltab)\n",
      "        \n",
      "    ##Create the delta a    \n",
      "    deltaa = deltah.dot(h.dot(1-h))\n",
      "        \n",
      "    ##Create delta V\n",
      "    deltav = np.outer(x,deltaa)\n",
      "    \n",
      "    return (deltaw,deltab,deltav,deltaa)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_iter_mlp(x_train,t_train,W,b,V,a):\n",
      "    learningrate = 0.0001\n",
      "    all_indices = np.arange(len(x_train),dtype=int)\n",
      "    np.random.shuffle(all_indices)\n",
      "        \n",
      "    for i in all_indices:\n",
      "        deltaw,deltab,deltav,deltaa = mlp_gradients(x_train[i],t_train[i],W,b,V,a)\n",
      "        W += learningrate * deltaw\n",
      "        b += learningrate * deltab\n",
      "        V += learningrate * deltav\n",
      "        a += learningrate * deltaa\n",
      "                \n",
      "    return (W,b,V,a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def check_mlp(x,t,W,b,V,a):    \n",
      "    h = 1 / (1 + np.exp(-(V.T.dot(x) + a)))\n",
      "        \n",
      "    logq = W.T.dot(h) + b\n",
      "        \n",
      "    p = np.exp(logq - np.log(np.sum(np.exp(logq))))\n",
      "    \n",
      "    return (np.argmax(p) == t, np.amax(p))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def checkset_mlp(X,T,W,b,V,A):\n",
      "    probDict = dict()\n",
      "    cL = []\n",
      "    for i in xrange(len(X)):\n",
      "        correctness, probability = check_mlp(X[i],T[i],W,b,V,a)\n",
      "        probDict[i] = probability\n",
      "        cL.append(correctness)\n",
      "        \n",
      "    return probDict,cL"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_mlp(L,iterations):\n",
      "    W = np.zeros([L,10])\n",
      "    b = np.zeros([10])\n",
      "    V = np.zeros([784,L])\n",
      "    a = np.zeros([L])\n",
      "    \n",
      "    for i in xrange(0,iterations):\n",
      "        W,b,V,a = sgd_iter_mlp(x_train,t_train,W,b,V,a)\n",
      "        \n",
      "    return W,b,V,a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#An example of how to train the MLP, note that these parameters do not give a very good result: about 32% correct.\n",
      "#In order to improve on this result, more iterations are necessary\n",
      "L = 20\n",
      "iterations = 10\n",
      "W,b,V,a = train_mlp(L,iterations)\n",
      "probDict, cL = checkset_mlp(x_valid,t_valid,W,b,V,a)\n",
      "print \"Validation error:\", 1-len([c for c in cL if c])/float(len(cL))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}